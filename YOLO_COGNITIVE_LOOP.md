# üéØ YOLO + Cognitive Loop - C√≥mo Probar el Sistema REAL

## üé• ¬øQu√© es esto?

Este es el **sistema COMPLETO** funcionando de verdad:
- ‚úÖ C√°mara real (tu webcam)
- ‚úÖ Detecci√≥n de objetos real (YOLOv8)
- ‚úÖ Sistema cognitivo real (IMA + Brain)
- ‚úÖ Decisiones reales en tiempo real

**NO es una simulaci√≥n** - el robot "ve" con YOLO y "piensa" con IMA.

---

## üì¶ Instalaci√≥n

### 1. Instalar dependencias

```powershell
# Desde el directorio ARIA con virtualenv activo
.\venv\Scripts\pip.exe install ultralytics opencv-python
```

**Paquetes instalados:**
- `ultralytics` - YOLOv8 (detecci√≥n de objetos)
- `opencv-python` - OpenCV (captura de c√°mara)

**Tama√±o:** ~200 MB (incluye modelo YOLO nano)

---

## üöÄ Uso B√°sico

### Demo con Webcam (Recomendado)

```powershell
# Modo simple
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo

# Solo dashboard (sin ventana de video)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --no-video
```

**Lo que ver√°s:**
- Ventana de video con detecciones YOLO en tiempo real
- Dashboard terminal con 4 paneles (Perception, World Model, IMA, Decision)
- Decisiones del robot basadas en lo que "ve"

**Controles:**
- `q` - Detener (en ventana de video)
- `Ctrl+C` - Detener (en terminal)

---

## üéÆ Modos de Uso

### 1. Webcam por Defecto

```powershell
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo
```

**Qu√© hace:**
- Abre tu webcam (c√°mara 0)
- Detecta objetos con YOLO nano (r√°pido)
- Muestra video + dashboard en tiempo real

---

### 2. Video File (Para Testing)

```powershell
# Procesar un video existente
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --video "C:\path\to\video.mp4"

# Solo 300 frames
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --video video.mp4 --max-frames 300
```

**√ötil para:**
- Probar sin c√°mara
- Videos de demostraci√≥n
- Testing reproducible

---

### 3. Modelo YOLO M√°s Preciso

```powershell
# Modelo small (m√°s preciso, m√°s lento)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --model-size s

# Modelo medium (muy preciso, mucho m√°s lento)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --model-size m
```

**Tama√±os de modelo:**
- `n` (nano) - R√°pido, 80 FPS, ~6 MB ‚úÖ **Recomendado**
- `s` (small) - Balanceado, 30 FPS, ~22 MB
- `m` (medium) - Preciso, 15 FPS, ~50 MB
- `l` (large) - Muy preciso, 8 FPS, ~90 MB
- `x` (xlarge) - M√°xima precisi√≥n, 3 FPS, ~136 MB

---

### 4. Ajustar Confianza

```powershell
# Detectar solo objetos con >70% confianza (menos detecciones, m√°s precisas)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --confidence 0.7

# Detectar objetos con >30% confianza (m√°s detecciones, menos precisas)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --confidence 0.3
```

---

## üìä Dashboard Explicado

### Panel 1: üì∏ Real Vision (YOLOv8)
- Objetos detectados en el frame actual
- Confidence de YOLO (0.0-1.0)
- **Novelty score** (üî¥ nuevo, üü¢ conocido)

### Panel 2: üó∫Ô∏è World Model (Memory)
- Objetos trackeados en memoria
- N√∫mero de veces que se vieron
- Novelty actualizada

### Panel 3: üß† IMA State (Motivation)
- **Energy**: Bater√≠a del robot (baja con el tiempo)
- **Temperature**: Temperatura interna
- **Pressure**: Qu√© tan mal est√° (0=OK, 1=CR√çTICO)
- **Novelty Drive**: Promedio de curiosidad actual

### Panel 4: üéØ Decision (Planning)
- **Action**: Qu√© decidi√≥ hacer el robot
- **Reasoning**: Por qu√© tom√≥ esa decisi√≥n
- **Priority**: Nivel de urgencia (0-3)

---

## üé¨ Comportamiento del Sistema

### Exploraci√≥n Normal (Energy > 30%)
```
üì∏ Detecta: person, car, chair
üó∫Ô∏è  Trackea: person visto 5 veces (novelty: 0.20)
üß† Energy: 85%, Pressure: 0.0
üéØ Decision: EXPLORE (priority: 0)
```

### Objeto Nuevo Detectado (Novelty > 0.5)
```
üì∏ Detecta: dog (PRIMERA VEZ)
üó∫Ô∏è  Trackea: dog visto 1 vez (novelty: 1.00) üî¥
üß† Novelty Drive: 0.85 (HIGH)
üéØ Decision: INSPECT_OBJECT -> dog (priority: 1)
```

### Energ√≠a Baja (10% < Energy < 20%)
```
üì∏ Detecta: person, car
üß† Energy: 15% üü° LOW
üß† Pressure: 0.50 (MEDIUM)
üéØ Decision: SEEK_CHARGING (priority: 2)
```

### Crisis de Energ√≠a (Energy < 10%)
```
üß† Energy: 8% üî¥ CRITICAL
üß† Pressure: 0.73 (HIGH)
üéØ Decision: SEEK_CHARGING - CRITICAL (priority: 3) üî¥üî¥üî¥
```

---

## üß™ Experimentos para Probar

### Experimento 1: Mostrar Objetos Nuevos
1. Inicia el sistema
2. Mu√©strale objetos comunes (taza, celular)
3. **Observa:** Novelty alta al principio (0.8-1.0)
4. Sigue mostrando los mismos objetos
5. **Observa:** Novelty baja con el tiempo (0.1-0.2)
6. Muestra algo nuevo (planta, libro)
7. **Observa:** Novelty salta a 1.0 ‚Üí Decision: INSPECT_OBJECT

### Experimento 2: Crisis de Energ√≠a
1. Inicia el sistema
2. Espera 2-3 minutos (energ√≠a bajando)
3. **Observa:** Energy pasa de 100% ‚Üí 50% ‚Üí 20%
4. Cuando Energy < 20%:
   - Decision cambia a SEEK_CHARGING
   - Priority aumenta a 2
5. Cuando Energy < 10%:
   - Priority aumenta a 3 (CRITICAL)
   - Homeostatic pressure > 0.7

### Experimento 3: Multiple Object Tracking
1. Muestra varios objetos simult√°neamente
2. **Observa:** World Model trackea m√∫ltiples entidades
3. Mueve objetos fuera del frame
4. **Observa:** Observations de esos objetos no aumentan
5. Vuelve a mostrarlos
6. **Observa:** Observations aumentan, novelty baja

---

## üìà M√©tricas al Final

Cuando detienes el sistema ver√°s:

```
======================================================================
REAL COGNITIVE LOOP SUMMARY
======================================================================

  Frames Processed    1,234
  Runtime             45.2s
  Average FPS         27.3

  Cognitive Loops     1,234
  Decisions Made      1,234
  Objects Tracked     15
  Unique Classes      8

  Final Energy        67.5%
  Final Temp          29.3¬∞C

======================================================================
‚úÖ Real cognitive loop with YOLO vision completed!
```

**Qu√© significan:**
- **Frames Processed**: Cu√°ntos frames de video se procesaron
- **Average FPS**: Velocidad de procesamiento
- **Cognitive Loops**: Cu√°ntas veces corri√≥ el loop sense‚Üíthink‚Üídecide
- **Objects Tracked**: Cu√°ntos objetos √∫nicos se trackearon
- **Unique Classes**: Cu√°ntos tipos de objetos diferentes se vieron

---

## üêõ Troubleshooting

### Error: "No module named 'ultralytics'"
```powershell
.\venv\Scripts\pip.exe install ultralytics
```

### Error: "No module named 'cv2'"
```powershell
.\venv\Scripts\pip.exe install opencv-python
```

### Error: "Failed to open camera 0"
```powershell
# Prueba con otra c√°mara
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --camera 1

# O usa un video
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --video video.mp4
```

### Video muy lento (FPS < 10)
```powershell
# Usa modelo nano (m√°s r√°pido)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --model-size n

# O reduce confianza (menos detecciones = m√°s r√°pido)
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --confidence 0.7
```

### Demasiadas detecciones
```powershell
# Aumenta threshold de confianza
.\venv\Scripts\python.exe -m aria_sdk.examples.cognitive_loop_yolo --confidence 0.6
```

---

## üéì Para el Paper

### Qu√© Citar

**Este demo prueba:**
1. ‚úÖ **Percepci√≥n Real** - YOLOv8 detecta objetos en tiempo real
2. ‚úÖ **IMA Funcional** - Novelty detection + homeostasis monitoring
3. ‚úÖ **World Model Funcional** - Tracking de entidades sobre el tiempo
4. ‚úÖ **Planning Funcional** - Decisiones basadas en prioridades
5. ‚úÖ **Cognitive Loop Completo** - Sense ‚Üí Think ‚Üí Decide en <50ms

**M√©tricas para el paper:**
- Latency del loop: ~30-40ms (25-30 FPS)
- Novelty decay: logar√≠tmico con exposure
- Decision latency: <1ms (despu√©s de perception)
- Memory efficiency: O(n) donde n = objetos √∫nicos

**Limitaciones conocidas:**
- Object tracking simple (by class, no multi-instance)
- No obstacle avoidance (safety layer es placeholder)
- No motor control (actions son simb√≥licas)
- Homeostasis es sint√©tico (no sensores reales de bater√≠a/temp)

---

## üî¨ Comparaci√≥n: Demo vs Real

| Feature | cognitive_loop_demo.py | cognitive_loop_yolo.py |
|---------|------------------------|------------------------|
| Perception | ‚ùå Simulada | ‚úÖ YOLO real |
| Objects | ‚ùå Random generation | ‚úÖ Real camera |
| IMA | ‚úÖ Funcional | ‚úÖ Funcional |
| Planning | ‚úÖ Funcional | ‚úÖ Funcional |
| Actions | ‚ùå Simuladas | ‚ùå Simuladas |
| Purpose | Probar IMA logic | Probar full system |

**Conclusi√≥n:** `cognitive_loop_yolo.py` es el sistema REAL end-to-end.

---

## üìö Objetos que YOLO Puede Detectar

YOLO nano puede detectar **80 clases** del dataset COCO:

```
person, bicycle, car, motorcycle, airplane, bus, train, truck, boat,
traffic light, fire hydrant, stop sign, parking meter, bench, bird,
cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack,
umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball,
kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket,
bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple,
sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair,
couch, potted plant, bed, dining table, toilet, tv, laptop, mouse,
remote, keyboard, cell phone, microwave, oven, toaster, sink,
refrigerator, book, clock, vase, scissors, teddy bear, hair drier,
toothbrush
```

**Para probar:** Mu√©strale tu celular, taza, laptop, libro, etc.

---

## üéØ Pr√≥ximos Pasos

1. ‚úÖ **LISTO**: Perception + IMA + Planning funcionando
2. ‚è≥ **TODO**: Motor control (enviar comandos a actuadores)
3. ‚è≥ **TODO**: Safety monitor (detectar obst√°culos/peligros)
4. ‚è≥ **TODO**: Reinforcement Learning (aprender de experiencia)
5. ‚è≥ **TODO**: Multi-modal (audio + vision)

---

## üí¨ Preguntas Frecuentes

**P: ¬øPor qu√© la energ√≠a baja si no hay robot f√≠sico?**
R: Es sint√©tico para demostrar homeostasis. En un robot real vendr√≠a de sensores de bater√≠a.

**P: ¬øPor qu√© las "acciones" no hacen nada?**
R: Porque no hay actuadores conectados. El demo muestra el proceso de decisi√≥n, no la ejecuci√≥n.

**P: ¬øPuedo usar mi GPU?**
R: S√≠, cambia `device='cpu'` a `device='cuda'` en `cognitive_loop_yolo.py` l√≠nea 79.

**P: ¬øFunciona en Linux/Mac?**
R: S√≠, solo ajusta los comandos (usa `python3` en vez de `.\venv\Scripts\python.exe`).

**P: ¬øPuedo usar otro modelo de detecci√≥n?**
R: S√≠, solo necesitas implementar el protocolo `Detection` en `yolo_detector.py`.

---

## üìù Nota Final

Este sistema demuestra que **la arquitectura ARIA funciona end-to-end**:
- Real perception (YOLO)
- Real cognitive processing (IMA)
- Real decision making (Planner)

**Es un proof-of-concept completo** listo para el paper. üéì‚ú®
