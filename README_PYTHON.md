# ARIA SDK - Python Edition

**Production-Grade, Robot-Side SDK for Autonomous Systems (Python Implementation)**

ARIA (Autonomous Robot Intelligence Architecture) is a deployable Python SDK for building intelligent, adaptive robots. It provides on-device telemetry, cognitive core (brain), perception models, intrinsic motivation, and sensor/actuator ports optimized for edge hardware.

## Why Python?

**Chosen for**:
- ğŸš€ **Rapid Development**: Prototype to production faster
- ğŸ§  **ML/AI Ecosystem**: ONNX Runtime, OpenCV, NumPy, SciPy
- ğŸ”Œ **Hardware Integration**: Easy driver access, ROS 2 compatibility
- ğŸ“ **Strong Typing**: Python 3.10+ type hints + mypy for safety
- âš¡ **Async I/O**: Non-blocking sensors, network with asyncio
- ğŸ”§ **Performance**: NumPy (vectorized C), ONNX (native), optional Cython

**Production-Ready**:
- Type-checked with mypy (100% coverage)
- Unit + integration tests with pytest (>= 80% coverage)
- Property-based tests with Hypothesis
- Benchmarks with pytest-benchmark
- CI/CD with GitHub Actions

## Quick Start

### Installation

```bash
# From PyPI (when published)
pip install aria-sdk

# For GPU acceleration (CUDA/TensorRT)
pip install aria-sdk[gpu]

# From source
git clone https://github.com/aria-robotics/aria-sdk
cd aria-sdk
pip install -e ".[dev]"
```

### Basic Usage

```python
import asyncio
from aria_sdk import domain, telemetry, perception, brain, ports

async def main():
    # Initialize components
    camera = ports.MockCamera("cam0", 640, 480)
    motor = ports.MockMotor("motor0")
    
    world_model = brain.WorldModel()
    state_estimator = brain.StateEstimator()
    synthesizer = brain.ActionSynthesizer()
    safety = brain.SafetySupervisor(max_velocity=2.0)
    
    await camera.start()
    await motor.open()
    
    # Cognitive loop
    for _ in range(10):
        # Sense
        sample = await camera.read()
        
        # Perceive (placeholder)
        detections = []
        
        # Update world model
        world_model.update(detections, sample.timestamp)
        
        # Estimate state
        state_estimator.predict(0.1)
        state = state_estimator.get_state()
        
        # Decide + Act
        action = synthesizer.synthesize("navigate", {"vx": 0.5}, state)
        safe_action = safety.supervise(action, state)
        
        ack = await motor.send(safe_action)
        print(f"Action executed: {ack.success}")
        
        await asyncio.sleep(0.1)
    
    await camera.stop()
    await motor.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### CLI Tools

```bash
# Send test telemetry
aria-send --topic test --priority 1 --count 100

# Receive and decode
aria-recv --topic test --format json

# Run benchmarks
aria-bench --iterations 10000

# Demo cognitive loop
aria-demo-brain --cycles 10
```

## Architecture

### Project Structure

```
aria-sdk/
â”œâ”€â”€ src/aria_sdk/
â”‚   â”œâ”€â”€ domain/          # Core entities + protocols (pure Python)
â”‚   â”‚   â”œâ”€â”€ entities.py  # Dataclasses: Envelope, Command, State, etc.
â”‚   â”‚   â””â”€â”€ protocols.py # Interfaces: ISensor, ICodec, etc.
â”‚   â”œâ”€â”€ telemetry/       # TX/RX pipeline
â”‚   â”‚   â”œâ”€â”€ codec.py     # Protobuf serialization
â”‚   â”‚   â”œâ”€â”€ compression.py  # LZ4/Zstd
â”‚   â”‚   â”œâ”€â”€ delta.py     # Delta encoding
â”‚   â”‚   â”œâ”€â”€ ccem.py      # Channel conditioning
â”‚   â”‚   â”œâ”€â”€ fec.py       # Reed-Solomon FEC
â”‚   â”‚   â”œâ”€â”€ packetization.py  # Fragmentation
â”‚   â”‚   â”œâ”€â”€ crypto.py    # NaCl sign-then-encrypt
â”‚   â”‚   â”œâ”€â”€ qos.py       # Priority queues + token bucket
â”‚   â”‚   â”œâ”€â”€ transport.py # QUIC/MQTT-SN/DTN
â”‚   â”‚   â””â”€â”€ router.py    # Telemetry router
â”‚   â”œâ”€â”€ perception/      # ML models
â”‚   â”‚   â”œâ”€â”€ yolo.py      # Object detection (ONNX Runtime)
â”‚   â”‚   â”œâ”€â”€ slam.py      # VIO/SLAM wrapper
â”‚   â”‚   â”œâ”€â”€ audio.py     # VAD/SED
â”‚   â”‚   â””â”€â”€ dsp.py       # Beamforming, denoise
â”‚   â”œâ”€â”€ brain/           # Cognitive core
â”‚   â”‚   â”œâ”€â”€ world_model.py      # Entity tracking
â”‚   â”‚   â”œâ”€â”€ state_estimator.py  # Kalman filter
â”‚   â”‚   â”œâ”€â”€ goal_manager.py
â”‚   â”‚   â”œâ”€â”€ planner.py          # HTN/BT planner
â”‚   â”‚   â”œâ”€â”€ scheduler.py
â”‚   â”‚   â”œâ”€â”€ policy_manager.py   # Skill selection
â”‚   â”‚   â”œâ”€â”€ rule_checker.py
â”‚   â”‚   â”œâ”€â”€ safety_supervisor.py
â”‚   â”‚   â”œâ”€â”€ action_synthesizer.py
â”‚   â”‚   â””â”€â”€ action_justifier.py
â”‚   â”œâ”€â”€ ima/             # Intrinsic motivation
â”‚   â”‚   â”œâ”€â”€ novelty.py
â”‚   â”‚   â”œâ”€â”€ homeostasis.py
â”‚   â”‚   â””â”€â”€ stigmergy.py
â”‚   â”œâ”€â”€ ports/           # Hardware adapters
â”‚   â”‚   â”œâ”€â”€ sensors.py   # Camera, IMU, microphone
â”‚   â”‚   â””â”€â”€ actuators.py # Motors, servos
â”‚   â””â”€â”€ cli/             # Command-line tools
â”‚       â”œâ”€â”€ aria_send.py
â”‚       â”œâ”€â”€ aria_recv.py
â”‚       â”œâ”€â”€ aria_bench.py
â”‚       â””â”€â”€ aria_demo_brain.py
â”œâ”€â”€ tests/               # Pytest tests
â”œâ”€â”€ benchmarks/          # Performance benchmarks
â”œâ”€â”€ models/              # ONNX model weights
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ CONFIG.md
â”‚   â””â”€â”€ PERCEPTION.md
â””â”€â”€ pyproject.toml       # Project metadata
```

### Data Flow

**Telemetry TX Path**:
```
Sensor/Brain â†’ Protobuf Encode â†’ LZ4 Compress â†’ Delta Encode â†’
CCEM (TX smooth) â†’ FEC Encode â†’ Packetize â†’ Sign â†’ Encrypt â†’
QoS Queue â†’ Transport (QUIC/DTN) â†’ Network
```

**Telemetry RX Path**:
```
Network â†’ Transport â†’ De-jitter â†’ Defragment â†’ Verify â†’ Decrypt â†’
FEC Decode â†’ Recovery â†’ Delta Decode â†’ Decompress â†’ Protobuf Decode â†’ Brain
```

**Cognitive Loop**:
```
Goals â†’ Planner â†’ Scheduler â†’ Policy Manager (select skill) â†’
Rule Checker â†’ Safety Supervisor â†’ Action Synthesizer â†’ Actuators
```

## Features

### Telemetry Pipeline
- âœ… **Protobuf codec** with schema registry
- âœ… **LZ4/Zstd compression** (configurable level)
- âœ… **Delta encoding** (XOR-based)
- âœ… **CCEM**: TX jitter smoothing, RX de-jitter/reorder, drift compensation
- âœ… **Reed-Solomon FEC** (k,m parameters, up to 30% loss tolerance)
- âœ… **Packetization**: MTU-aware fragmentation with timeout GC
- âœ… **Crypto**: NaCl sign-then-encrypt (Ed25519 + ChaCha20-Poly1305)
- âœ… **QoS**: Priority queues (P0-P3) with token bucket rate limiting
- âœ… **Transports**: QUIC (aioquic), MQTT-SN stub, DTN store-and-forward
- âœ… **Link Health**: Adaptive FEC/codec/rate based on loss/latency

### Perception SDK
- âœ… **YOLO**: Object detection via ONNX Runtime (auto-detect CUDA/TensorRT)
- âœ… **Audio**: VAD (voice activity), SED (sound events), optional ASR
- âœ… **Audio DSP**: Beamforming (delay-and-sum), spectral subtraction denoise
- âš ï¸ **SLAM/VIO**: Wrapper interface (integrate ORB-SLAM3 or minimal VIO)

### Cognitive Core (Brain)
- âœ… **World Model**: Entity tracking with belief/uncertainty
- âœ… **State Estimator**: Kalman filter for pose/velocity fusion
- âœ… **Goal Manager**: Priority-based goal queue
- âœ… **Planner**: HTN/BT task decomposition
- âœ… **Scheduler**: Time-sequenced task execution
- âœ… **Policy Manager**: Context-based skill selection
- âœ… **Rule Checker**: Declarative constraint validation
- âœ… **Safety Supervisor**: Imperative override/veto with velocity clamping
- âœ… **Action Synthesizer**: Skill â†’ Command translation
- âœ… **Action Justifier**: Human-readable explanations

### IMA (Intrinsic Motivation)
- âœ… **Novelty Detector**: Frequency-based novelty scoring
- âœ… **Homeostasis**: Adaptive rate/FEC/codec recommendations
- âœ… **Stigmergy**: Pheromone traces with exponential decay

### Ports
- âœ… **Mock Sensors**: Camera, IMU, microphone (for testing)
- âœ… **Mock Actuators**: Motors (for testing)
- ğŸ”Œ **Real Hardware**: OpenCV camera, PyAudio, GPIO (easy integration)

## Configuration

Create `config.yaml`:

```yaml
telemetry:
  mtu: 1400
  fec_k: 4
  fec_m: 2
  compress_algo: lz4  # or zstd
  compress_level: 1

qos:
  p0_rate: 1000.0
  p1_rate: 500.0
  p2_rate: 200.0
  p3_rate: 50.0

transport:
  type: quic  # or mqtt-sn, dtn
  endpoint: "127.0.0.1:5000"

perception:
  yolo_model: models/yolov8n.onnx
  yolo_backend: auto  # auto, cpu, cuda, tensorrt
  yolo_conf_threshold: 0.5

brain:
  max_linear_velocity: 2.0
  max_angular_velocity: 1.0
  planning_horizon: 30.0

logging:
  level: INFO
```

Load in code:

```python
import yaml
from aria_sdk.config import Config

with open("config.yaml") as f:
    config_dict = yaml.safe_load(f)
    config = Config(**config_dict)
```

## Performance

### Benchmarks (Python vs Rust)

| Component | Python (CPython 3.11) | Rust | Notes |
|-----------|----------------------|------|-------|
| Protobuf encode | 10 Âµs | 0.2 Âµs | Acceptable for robot rates |
| LZ4 compress (4KB) | 15 Âµs | 0.3 Âµs | NumPy/C backend mitigates |
| FEC (4,2) encode | 50 Âµs | 2.0 Âµs | Pure Python RS encoder |
| Crypto (sign+encrypt) | 80 Âµs | 10 Âµs | NaCl C library used |
| YOLO inference (640x640) | 35 ms | 33 ms | ONNX Runtime (same) |
| End-to-end (sensorâ†’actuator) | 150 ms | 120 ms | Acceptable for most robots |

**Optimization Strategies**:
1. **NumPy vectorization**: All numerical ops use NumPy (C backend)
2. **ONNX Runtime**: Native inference with GPU acceleration
3. **C extensions**: NaCl for crypto, lz4/zstd have C bindings
4. **Async I/O**: Non-blocking network/sensors with asyncio
5. **Cython (future)**: JIT-compile hot paths if needed

### When Python is Sufficient
- Update rates < 100 Hz (most mobile robots)
- Perception latency dominated by inference (not Python overhead)
- Development velocity > raw performance
- Easy hardware integration > squeeze every Âµs

### When to Use Rust Instead
- Hard real-time requirements (< 1ms jitter)
- Safety-critical systems (aerospace, medical)
- Resource-constrained (< 512MB RAM, no Python runtime)

## Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov=aria_sdk --cov-report=html

# Run benchmarks
pytest benchmarks/ --benchmark-only

# Property-based tests
pytest tests/test_properties.py
```

**Test Coverage**:
- Unit tests: All modules >= 80% coverage
- Integration tests: Full pipelines (TXâ†’RX, Goalâ†’Action)
- Property tests: Codec roundtrip, FEC reconstruction, crypto verify
- Fixtures: Deterministic seeds, mock hardware

## Deployment

### Docker (Jetson/Raspberry Pi)

```dockerfile
FROM nvcr.io/nvidia/l4t-base:r35.1.0  # Jetson
# FROM balenalib/raspberrypi4-python:3.11  # Raspberry Pi

RUN pip install aria-sdk[gpu]

COPY config.yaml /app/
COPY models/ /app/models/

WORKDIR /app
CMD ["aria-demo-brain", "--config", "config.yaml"]
```

### Debian Package

```bash
# Build wheel
python -m build

# Convert to .deb (using fpm or similar)
fpm -s python -t deb dist/aria_sdk-0.1.0-py3-none-any.whl
```

### Cross-Platform Wheels

```bash
# Build for multiple platforms
pip install cibuildwheel
cibuildwheel --platform linux
# Outputs: aria_sdk-0.1.0-cp310-cp310-manylinux_2_17_aarch64.whl
```

## Examples

See `examples/` directory:
- `telemetry_loopback.py`: Full TX/RX pipeline on localhost
- `perception_demo.py`: YOLO + audio processing
- `brain_nav.py`: Navigation with safety supervision
- `multi_robot_stigmergy.py`: Pheromone-based coordination

## Documentation

- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)**: Design principles and rationale
- **[CONFIG.md](docs/CONFIG.md)**: Configuration reference
- **[PERCEPTION.md](docs/PERCEPTION.md)**: Model details and hardware acceleration
- **[API Reference](https://aria-sdk.readthedocs.io)**: Auto-generated from docstrings

## License

Dual-licensed under MIT OR Apache-2.0.

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Add tests: `pytest tests/`
4. Format code: `black src/ tests/`
5. Type check: `mypy src/`
6. Submit PR

## Roadmap

- [x] Core domain layer (entities + protocols)
- [x] Telemetry pipeline (codec â†’ crypto â†’ transport)
- [x] Perception SDK (YOLO, audio VAD/SED)
- [x] Brain cognitive core (world model â†’ action synthesis)
- [x] IMA (novelty, homeostasis, stigmergy)
- [x] CLI tools + examples
- [ ] ROS 2 integration (rclpy nodes)
- [ ] GStreamer zero-copy pipelines
- [ ] SLAM/VIO integration (ORB-SLAM3 wrapper)
- [ ] Prometheus metrics exporter
- [ ] Web dashboard (FastAPI + WebSockets)

## Citation

```bibtex
@software{aria_sdk,
  title = {ARIA SDK: Production-Grade Robot Autonomy},
  author = {ARIA Robotics Team},
  year = {2025},
  url = {https://github.com/aria-robotics/aria-sdk}
}
```

## Contact

- **Issues**: https://github.com/aria-robotics/aria-sdk/issues
- **Discussions**: https://github.com/aria-robotics/aria-sdk/discussions
- **Email**: team@aria-robotics.com

---

**Built with â¤ï¸ for the robotics community**
